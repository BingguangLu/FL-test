{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Beginners Guide to Federated Learning\n",
    "\n",
    "Follow steps in https://towardsdatascience.com/federated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399\n",
    "\n",
    "## Setting Up the Environment and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from FL_initial_test_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/42000\n",
      "[INFO] processed 20000/42000\n",
      "[INFO] processed 30000/42000\n",
      "[INFO] processed 40000/42000\n"
     ]
    }
   ],
   "source": [
    "#declear path to your mnist data folder\n",
    "img_path = 'data/MNIST/trainingSet/trainingSet'\n",
    "\n",
    "#get the path list using the path object\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "\n",
    "#apply our function\n",
    "image_list, label_list = load(image_paths, verbose=10000)\n",
    "\n",
    "#binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "label_list = lb.fit_transform(label_list)\n",
    "\n",
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
    "                                                    label_list, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lubingguang/miniconda3/envs/tensorflow/lib/python3.8/site-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 \n",
    "comms_round = 100\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 0 | global_acc: 87.952% | global_loss: 1.669370174407959\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 1 | global_acc: 90.262% | global_loss: 1.6220005750656128\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 2 | global_acc: 91.214% | global_loss: 1.602520227432251\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 3 | global_acc: 92.238% | global_loss: 1.5920063257217407\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 4 | global_acc: 92.595% | global_loss: 1.5834453105926514\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 5 | global_acc: 92.929% | global_loss: 1.5755285024642944\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 6 | global_acc: 93.524% | global_loss: 1.5689549446105957\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 7 | global_acc: 93.619% | global_loss: 1.5646910667419434\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 8 | global_acc: 93.881% | global_loss: 1.56080162525177\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 9 | global_acc: 94.048% | global_loss: 1.5576765537261963\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 10 | global_acc: 94.143% | global_loss: 1.5545189380645752\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 11 | global_acc: 94.048% | global_loss: 1.55177640914917\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 12 | global_acc: 94.500% | global_loss: 1.5493052005767822\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 13 | global_acc: 94.548% | global_loss: 1.5474591255187988\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 14 | global_acc: 94.762% | global_loss: 1.5454508066177368\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 15 | global_acc: 94.643% | global_loss: 1.5437490940093994\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 16 | global_acc: 94.833% | global_loss: 1.5422642230987549\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 17 | global_acc: 94.881% | global_loss: 1.5406063795089722\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 18 | global_acc: 94.976% | global_loss: 1.5397391319274902\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 19 | global_acc: 95.119% | global_loss: 1.538509488105774\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 20 | global_acc: 95.119% | global_loss: 1.5373188257217407\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 21 | global_acc: 95.143% | global_loss: 1.536424160003662\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 22 | global_acc: 95.119% | global_loss: 1.5354403257369995\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 23 | global_acc: 95.310% | global_loss: 1.5342121124267578\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 24 | global_acc: 95.190% | global_loss: 1.5335588455200195\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 25 | global_acc: 95.357% | global_loss: 1.5328049659729004\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 26 | global_acc: 95.429% | global_loss: 1.532329797744751\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 27 | global_acc: 95.500% | global_loss: 1.5314018726348877\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 28 | global_acc: 95.310% | global_loss: 1.5308856964111328\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 29 | global_acc: 95.381% | global_loss: 1.5300887823104858\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 30 | global_acc: 95.405% | global_loss: 1.529510498046875\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 31 | global_acc: 95.405% | global_loss: 1.5291221141815186\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 32 | global_acc: 95.619% | global_loss: 1.5281710624694824\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 33 | global_acc: 95.333% | global_loss: 1.528102993965149\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 34 | global_acc: 95.500% | global_loss: 1.5273346900939941\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 35 | global_acc: 95.643% | global_loss: 1.526668906211853\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 36 | global_acc: 95.690% | global_loss: 1.5263394117355347\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 37 | global_acc: 95.905% | global_loss: 1.5261799097061157\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 38 | global_acc: 95.714% | global_loss: 1.525389313697815\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 39 | global_acc: 95.810% | global_loss: 1.5250062942504883\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 40 | global_acc: 95.810% | global_loss: 1.5246909856796265\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 41 | global_acc: 95.881% | global_loss: 1.5243377685546875\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 42 | global_acc: 95.810% | global_loss: 1.523903250694275\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 43 | global_acc: 95.833% | global_loss: 1.5235663652420044\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 44 | global_acc: 95.929% | global_loss: 1.52345609664917\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 45 | global_acc: 95.929% | global_loss: 1.5228546857833862\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 46 | global_acc: 95.857% | global_loss: 1.5226224660873413\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 47 | global_acc: 96.048% | global_loss: 1.5224946737289429\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 48 | global_acc: 95.976% | global_loss: 1.5219882726669312\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 49 | global_acc: 95.952% | global_loss: 1.5218143463134766\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 50 | global_acc: 95.976% | global_loss: 1.5214753150939941\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 51 | global_acc: 96.000% | global_loss: 1.52128005027771\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 52 | global_acc: 96.071% | global_loss: 1.5209362506866455\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 53 | global_acc: 96.167% | global_loss: 1.5206817388534546\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 54 | global_acc: 96.095% | global_loss: 1.5204726457595825\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 55 | global_acc: 96.071% | global_loss: 1.520196795463562\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 56 | global_acc: 96.095% | global_loss: 1.5199501514434814\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 57 | global_acc: 96.143% | global_loss: 1.5197862386703491\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 58 | global_acc: 96.071% | global_loss: 1.5194580554962158\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 59 | global_acc: 96.071% | global_loss: 1.5192533731460571\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 60 | global_acc: 96.167% | global_loss: 1.519042730331421\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 61 | global_acc: 96.048% | global_loss: 1.5188947916030884\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 62 | global_acc: 96.167% | global_loss: 1.518604040145874\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 63 | global_acc: 96.143% | global_loss: 1.518451452255249\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 64 | global_acc: 96.190% | global_loss: 1.5183627605438232\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 65 | global_acc: 96.214% | global_loss: 1.5181456804275513\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 66 | global_acc: 96.214% | global_loss: 1.5180033445358276\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 67 | global_acc: 96.214% | global_loss: 1.5177583694458008\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 68 | global_acc: 96.095% | global_loss: 1.5175729990005493\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 69 | global_acc: 96.095% | global_loss: 1.5173512697219849\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 70 | global_acc: 96.190% | global_loss: 1.5173094272613525\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 71 | global_acc: 96.238% | global_loss: 1.517051100730896\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 72 | global_acc: 96.238% | global_loss: 1.5168930292129517\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 73 | global_acc: 96.190% | global_loss: 1.5168057680130005\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 74 | global_acc: 96.214% | global_loss: 1.5165480375289917\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 75 | global_acc: 96.143% | global_loss: 1.5164011716842651\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 76 | global_acc: 96.310% | global_loss: 1.51626718044281\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 77 | global_acc: 96.238% | global_loss: 1.5161163806915283\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 78 | global_acc: 96.214% | global_loss: 1.5160802602767944\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 79 | global_acc: 96.262% | global_loss: 1.5158723592758179\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 80 | global_acc: 96.286% | global_loss: 1.5157568454742432\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 81 | global_acc: 96.238% | global_loss: 1.5155048370361328\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 82 | global_acc: 96.238% | global_loss: 1.515421748161316\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 83 | global_acc: 96.143% | global_loss: 1.5153409242630005\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 84 | global_acc: 96.238% | global_loss: 1.5152840614318848\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 85 | global_acc: 96.262% | global_loss: 1.5151047706604004\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 86 | global_acc: 96.262% | global_loss: 1.5150558948516846\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 87 | global_acc: 96.190% | global_loss: 1.5148335695266724\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 88 | global_acc: 96.238% | global_loss: 1.5146938562393188\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 89 | global_acc: 96.238% | global_loss: 1.5146024227142334\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 90 | global_acc: 96.262% | global_loss: 1.514471173286438\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 91 | global_acc: 96.214% | global_loss: 1.5143452882766724\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 92 | global_acc: 96.286% | global_loss: 1.5142254829406738\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 93 | global_acc: 96.310% | global_loss: 1.5141996145248413\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 94 | global_acc: 96.286% | global_loss: 1.514096736907959\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 95 | global_acc: 96.238% | global_loss: 1.5139349699020386\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 96 | global_acc: 96.214% | global_loss: 1.5138390064239502\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 97 | global_acc: 96.262% | global_loss: 1.513708233833313\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 98 | global_acc: 96.333% | global_loss: 1.5137015581130981\n",
      "132/132 [==============================] - 0s 1ms/step\n",
      "comm_round: 99 | global_acc: 96.286% | global_loss: 1.5135911703109741\n"
     ]
    }
   ],
   "source": [
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(784, 10)\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
